{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import plot_importance\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler\n",
    "\n",
    "from sklearn.model_selection import KFold,TimeSeriesSplit\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv(\"./train.csv\", index_col=0)\n",
    "test = pd.read_csv(\"./test.csv\", index_col=0).reset_index(drop=True)\n",
    "submission = pd.read_csv(\"./sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['u-g'] = train['u'] - train['g']\n",
    "train['g-r'] = train['g'] - train['r']\n",
    "train['r-i'] = train['r'] - train['i']\n",
    "train['i-z'] = train['i'] - train['z']\n",
    "train['dered_u-g'] = train['dered_u'] - train['dered_g']\n",
    "train['dered_g-r'] = train['dered_g'] - train['dered_r']\n",
    "train['dered_r-i'] = train['dered_r'] - train['dered_i']\n",
    "train['dered_i-z'] = train['dered_i'] - train['dered_z']\n",
    "\n",
    "test['u-g'] = test['u'] - test['g']\n",
    "test['g-r'] = test['g'] - test['r']\n",
    "test['r-i'] = test['r'] - test['i']\n",
    "test['i-z'] = test['i'] - test['z']\n",
    "test['dered_u-g'] = test['dered_u'] - test['dered_g']\n",
    "test['dered_g-r'] = test['dered_g'] - test['dered_r']\n",
    "test['dered_r-i'] = test['dered_r'] - test['dered_i']\n",
    "test['dered_i-z'] = test['dered_i'] - test['dered_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['r_u-g'] = train['redshift'] * train['u-g']\n",
    "train['r_g-r'] = train['redshift'] * train['g-r']\n",
    "train['r_r-i'] = train['redshift'] * train['r-i']\n",
    "train['r_i-z'] = train['redshift'] * train['i-z']\n",
    "train['r_dered_u-g'] = train['redshift'] * train['dered_u-g']\n",
    "train['r_dered_g-r'] = train['redshift'] * train['dered_g-r']\n",
    "train['r_dered_r-i'] = train['redshift'] * train['dered_r-i']\n",
    "train['r_dered_i-z'] = train['redshift'] * train['dered_i-z']\n",
    "\n",
    "test['r_u-g'] = test['redshift'] * test['u-g']\n",
    "test['r_g-r'] = test['redshift'] * test['g-r']\n",
    "test['r_r-i'] = test['redshift'] * test['r-i']\n",
    "test['r_i-z'] = test['redshift'] * test['i-z']\n",
    "test['r_dered_u-g'] = test['redshift'] * test['dered_u-g']\n",
    "test['r_dered_g-r'] = test['redshift'] * test['dered_g-r']\n",
    "test['r_dered_r-i'] = test['redshift'] * test['dered_r-i']\n",
    "test['r_dered_i-z'] = test['redshift'] * test['dered_i-z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_train = train[[\"u\", \"g\", \"r\", \"i\", \"z\"]]\n",
    "dered_train = train[[\"dered_u\", \"dered_g\", \"dered_r\", \"dered_i\", \"dered_z\"]]\n",
    "\n",
    "rare_test = test[[\"u\", \"g\", \"r\", \"i\", \"z\"]]\n",
    "dered_test = test[[\"dered_u\", \"dered_g\", \"dered_r\", \"dered_i\", \"dered_z\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"rare_mean\"] = rare_train.mean(axis = 1)\n",
    "train[\"rare_var\"] = rare_train.var(axis = 1)\n",
    "train[\"rare_std\"] = rare_train.std(axis = 1)\n",
    "train[\"rare_sum\"] = rare_train.sum(axis = 1)\n",
    "train[\"rare_median\"] = rare_train.median(axis = 1)\n",
    "train[\"rare_max\"] = rare_train.max(axis = 1)\n",
    "train['dered_mean'] = dered_train.mean(axis=1)\n",
    "train['dered_var'] = dered_train.var(axis=1)\n",
    "train['dered_std'] = dered_train.std(axis=1)\n",
    "train['dered_sum'] = dered_train.sum(axis=1)\n",
    "train['dered_median'] = dered_train.median(axis=1)\n",
    "train[\"dered_max\"] = dered_train.max(axis = 1)\n",
    "\n",
    "test[\"rare_mean\"] = rare_test.mean(axis = 1)\n",
    "test[\"rare_var\"] = rare_test.var(axis = 1)\n",
    "test[\"rare_std\"] = rare_test.std(axis = 1)\n",
    "test[\"rare_sum\"] = rare_test.sum(axis = 1)\n",
    "test[\"rare_median\"] = rare_test.median(axis = 1)\n",
    "test[\"rare_max\"] = rare_test.max(axis = 1)\n",
    "test['dered_mean'] = dered_test.mean(axis=1)\n",
    "test['dered_var'] = dered_test.var(axis=1)\n",
    "test['dered_std'] = dered_test.std(axis=1)\n",
    "test['dered_sum'] = dered_test.sum(axis=1)\n",
    "test['dered_median'] = dered_test.median(axis=1)\n",
    "test[\"dered_max\"] = dered_test.max(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://classic.sdss.org/education/kron_ARCS.pdf\n",
    "distance_train = []\n",
    "for rs in train['redshift']:\n",
    "    if rs > 0:\n",
    "        distance_train.append((rs / (1+rs)) * 13.5 * 10**9)\n",
    "    else:\n",
    "        distance_train.append((abs(rs-1) / abs(rs)) * 13.5 * 10**9)\n",
    "\n",
    "train['distance'] = distance_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://classic.sdss.org/education/kron_ARCS.pdf\n",
    "distance_test = []\n",
    "for rs in test['redshift']:\n",
    "    if rs > 0:\n",
    "        distance_test.append((rs / (1+rs)) * 13.5 * 10**9)\n",
    "    else:\n",
    "        distance_test.append((abs(rs-1) / abs(rs)) * 13.5 * 10**9)\n",
    "\n",
    "test['distance'] = distance_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n값 처리\n",
    "train['nO-nD'] = train['nObserve'] - train['nDetect']\n",
    "test['nO-nD'] = test['nObserve'] - test['nDetect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 나눗셈\n",
    "train['u/dered_u'] = train['u'] / train['dered_u'] - 1\n",
    "train['g/dered_g'] = train['g'] / train['dered_g'] - 1\n",
    "train['r/dered_r'] = train['r'] / train['dered_r'] - 1\n",
    "train['i/dered_i'] = train['i'] / train['dered_i'] - 1\n",
    "train['z/dered_z'] = train['z'] / train['dered_z'] - 1\n",
    "\n",
    "test['u/dered_u'] = test['u'] / test['dered_u'] - 1\n",
    "test['g/dered_g'] = test['g'] / test['dered_g'] - 1\n",
    "test['r/dered_r'] = test['r'] / test['dered_r'] - 1\n",
    "test['i/dered_i'] = test['i'] / test['dered_i'] - 1\n",
    "test['z/dered_z'] = test['z'] / test['dered_z'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['M_u'] = train['u'] - 5*(np.log(train['distance']) - 1)\n",
    "train['M_g'] = train['g'] - 5*(np.log(train['distance']) - 1)\n",
    "train['M_r'] = train['r'] - 5*(np.log(train['distance']) - 1)\n",
    "train['M_i'] = train['i'] - 5*(np.log(train['distance']) - 1)\n",
    "train['M_z'] = train['z'] - 5*(np.log(train['distance']) - 1)\n",
    "train['M'] = train['rare_max'] - 5*(np.log(train['distance']) - 1)\n",
    "train['M_dered_u'] = train['dered_u'] - 5*(np.log(train['distance']) - 1)\n",
    "train['M_dered_g'] = train['dered_g'] - 5*(np.log(train['distance']) - 1)\n",
    "train['M_dered_r'] = train['dered_r'] - 5*(np.log(train['distance']) - 1)\n",
    "train['M_dered_i'] = train['dered_i'] - 5*(np.log(train['distance']) - 1)\n",
    "train['M_dered_z'] = train['dered_z'] - 5*(np.log(train['distance']) - 1)\n",
    "train['dered_M'] = train['dered_max'] - 5*(np.log(train['distance']) - 1)\n",
    "\n",
    "test['M_u'] = test['u'] - 5*(np.log(test['distance']) - 1)\n",
    "test['M_g'] = test['g'] - 5*(np.log(test['distance']) - 1)\n",
    "test['M_r'] = test['r'] - 5*(np.log(test['distance']) - 1)\n",
    "test['M_i'] = test['i'] - 5*(np.log(test['distance']) - 1)\n",
    "test['M_z'] = test['z'] - 5*(np.log(test['distance']) - 1)\n",
    "test['M'] = test['rare_max'] - 5*(np.log(test['distance']) - 1)\n",
    "test['M_dered_u'] = test['dered_u'] - 5*(np.log(test['distance']) - 1)\n",
    "test['M_dered_g'] = test['dered_g'] - 5*(np.log(test['distance']) - 1)\n",
    "test['M_dered_r'] = test['dered_r'] - 5*(np.log(test['distance']) - 1)\n",
    "test['M_dered_i'] = test['dered_i'] - 5*(np.log(test['distance']) - 1)\n",
    "test['M_dered_z'] = test['dered_z'] - 5*(np.log(test['distance']) - 1)\n",
    "test['dered_M'] = test['dered_max'] - 5*(np.log(test['distance']) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train['class']\n",
    "del train['class']\n",
    "X = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "eval_set = [(X_val, y_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    time1 = time.time()\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'subsample': \"{:.2f}\".format(params['subsample']),\n",
    "        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n",
    "        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n",
    "        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
    "        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n",
    "        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n",
    "        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n",
    "    }\n",
    "\n",
    "    print(\"\\n############## New Run ################\")\n",
    "    print(f\"params = {params}\")\n",
    "    FOLDS = 5\n",
    "    count=1\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=FOLDS)\n",
    "    y_preds = np.zeros(submission.shape[0])\n",
    "    y_oof = np.zeros(X_train.shape[0])\n",
    "    score_mean = 0\n",
    "    for tr_idx, val_idx in tss.split(X_train, y_train):\n",
    "        clf = xgb.XGBClassifier(\n",
    "            objective='multi:softmax',\n",
    "            num_class=3,\n",
    "            eval_metric='merror',\n",
    "            n_estimators=1000,\n",
    "            random_state=4,\n",
    "            verbose=True, \n",
    "            tree_method='gpu_hist', \n",
    "            **params\n",
    "        )\n",
    "\n",
    "        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n",
    "        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        clf.fit(X_tr, y_tr)\n",
    "        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n",
    "        #print(y_pred_train)\n",
    "        score = make_scorer(accuracy_score, needs_proba=True)(clf, X_vl, y_vl)\n",
    "        # plt.show()\n",
    "        score_mean += score\n",
    "        print(f'{count} CV - score: {round(score, 4)}')\n",
    "        count += 1\n",
    "    time2 = time.time() - time1\n",
    "    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
    "    gc.collect()\n",
    "    print(f'Mean accuracy: {score_mean / FOLDS}')\n",
    "    del X_tr, X_vl, y_tr, y_vl, clf, score\n",
    "    return -(score_mean / FOLDS)\n",
    "\n",
    "\n",
    "space = {\n",
    "    # The maximum depth of a tree, same as GBM.\n",
    "    # Used to control over-fitting as higher depth will allow model \n",
    "    # to learn relations very specific to a particular sample.\n",
    "    # Should be tuned using CV.\n",
    "    # Typical values: 3-10\n",
    "    'max_depth': hp.quniform('max_depth', 10, 23, 1),\n",
    "    \n",
    "    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n",
    "    # (meaning pulling weights to 0). It can be more useful when the objective\n",
    "    # is logistic regression since you might need help with feature selection.\n",
    "    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n",
    "    \n",
    "    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n",
    "    # approach can be more useful in tree-models where zeroing \n",
    "    # features might not make much sense.\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n",
    "    \n",
    "    # eta: Analogous to learning rate in GBM\n",
    "    # Makes the model more robust by shrinking the weights on each step\n",
    "    # Typical final values to be used: 0.01-0.2\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "    \n",
    "    # colsample_bytree: Similar to max_features in GBM. Denotes the \n",
    "    # fraction of columns to be randomly samples for each tree.\n",
    "    # Typical values: 0.5-1\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n",
    "    \n",
    "    # A node is split only when the resulting split gives a positive\n",
    "    # reduction in the loss function. Gamma specifies the \n",
    "    # minimum loss reduction required to make a split.\n",
    "    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "    'gamma': hp.uniform('gamma', 0.01, .7),\n",
    "    \n",
    "    # more increases accuracy, but may lead to overfitting.\n",
    "    # num_leaves: the number of leaf nodes to use. Having a large number \n",
    "    # of leaves will improve accuracy, but will also lead to overfitting.\n",
    "    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n",
    "    \n",
    "    # specifies the minimum samples per leaf node.\n",
    "    # the minimum number of samples (data) to group into a leaf. \n",
    "    # The parameter can greatly assist with overfitting: larger sample\n",
    "    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n",
    "    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n",
    "    \n",
    "    # subsample: represents a fraction of the rows (observations) to be \n",
    "    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n",
    "    # in their paper A Scalable Tree Boosting System recommend \n",
    "    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n",
    "    \n",
    "    # randomly select a fraction of the features.\n",
    "    # feature_fraction: controls the subsampling of features used\n",
    "    # for training (as opposed to subsampling the actual training data in \n",
    "    # the case of bagging). Smaller fractions reduce overfitting.\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n",
    "    \n",
    "    # randomly bag or subsample training data.\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n",
    "    \n",
    "    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n",
    "    # of the training data. Both values need to be set for bagging to be used.\n",
    "    # The frequency controls how often (iteration) bagging is used. Smaller\n",
    "    # fractions and frequencies reduce overfitting.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set algoritm parameters\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=27)\n",
    "\n",
    "# Print best parameters\n",
    "best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEST PARAMS: \", best_params)\n",
    "\n",
    "best_params['max_depth'] = int(best_params['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
